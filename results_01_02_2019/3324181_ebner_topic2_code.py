# -*- coding: utf-8 -*-
"""DL_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RKp-RWZ5pl113Rt_0XM7v1qxDZPEDo3G
"""

################################
from google.colab import drive
drive.mount('/content/gdrive')

################################
# save data in better form 
#(only once, only relevant data, concat features from logmel and prosody)
import numpy as np
import pickle

##dictionary for translating labels
labelDic = {'angry': 0, 'happy': 1, 'sad': 2, 'neutral': 3}

# Load logmel for training
f = open("data_logmel.train", "rb")
pickle.load(f)  # ignore ids
train_features = pickle.load(f)
train_labels = pickle.load(f)
train_labels = [labelDic[label] for label in train_labels]
f.close()
# load prosody for training
f = open("data_prosody.train", "rb")
pickle.load(f)  # ignore ids
train_features = np.concatenate((train_features, pickle.load(f)), axis=2)
pickle.load(f)  # ignore labels, they are the same
f.close()

# Load logmel for validation
f = open("data_logmel.valid", "rb")
pickle.load(f)  # ignore ids
valid_features = pickle.load(f)
valid_labels = pickle.load(f)
valid_labels = [labelDic[label] for label in valid_labels]
f.close()
# load prosody for validation
f = open("data_prosody.valid", "rb")
pickle.load(f)  # ignore ids
valid_features = np.concatenate((valid_features, pickle.load(f)), axis=2)
pickle.load(f)  # ignore labels, they are the same
f.close()

# Load logmel for testing
f = open("data_logmel.test", "rb")
pickle.load(f)  # ignore ids
test_features = pickle.load(f)
f.close()
# load prosody for testing
f = open("data_prosody.test", "rb")
test_ids = pickle.load(f)  # ignore ids
test_features = np.concatenate((test_features, pickle.load(f)), axis=2)
f.close()

# save in better form
with open("data_project", 'wb') as f_out:
    pickle.dump([train_features, train_labels, valid_features, valid_labels, test_features, test_ids], f_out)

################################
# define dimensions
num_frames = 750
num_feats = 33
num_classes = 4

################################
# Load the data
import pickle 
import numpy as np

loadTrueData = True
if loadTrueData:
    f = open('/content/gdrive/My Drive/DL_project/data_project', "rb")
    [train_features_3dim, train_labels_int, valid_features_3dim, valid_labels_int, test_features_3dim, test_ids] = pickle.load(f,encoding='latin1')
else:
    noSamples = 20
    train_features_3dim = np.ones((noSamples,num_frames,num_feats))
    train_labels_int = np.random.randint(num_classes, size = noSamples)
    valid_features_3dim = np.ones((noSamples,num_frames))
    valid_labels_int = np.random.randint(num_classes, size = noSamples)
    test_features_3dim = np.ones((noSamples,num_frames))

################################
# prepare the data
from keras.utils import to_categorical
import numpy as np
train_labels = to_categorical(train_labels_int)
valid_labels = to_categorical(valid_labels_int)
train_features = train_features_3dim[..., np.newaxis]
valid_features = valid_features_3dim[..., np.newaxis]
test_features = test_features_3dim[..., np.newaxis]

print(train_features.shape)
print( train_labels.shape)
print( valid_features.shape)
print( valid_labels.shape)
print( test_features.shape)



################################
# Create a sequential model
from keras.models import Sequential
from keras.layers import Dense, Conv1D, Conv2D, Flatten, Dropout
from keras.callbacks import EarlyStopping
from keras import regularizers
from talos.model.early_stopper import early_stopper
from talos.model.normalizers import lr_normalizer

def dl_model(x_train, y_train, x_val, y_val, params):


  model = Sequential()
  model.add(Conv2D(params['filters_first_conv'], kernel_size=(12,7), strides=(7,4),activation='relu', input_shape=(num_frames,num_feats,1)))
  model.add(Dropout(params['dropout']))
  model.add(Conv2D(params['filters_second_conv'], kernel_size=5,strides=(2,2),activation='relu', input_shape=(num_frames,num_feats,1)))
  model.add(Dropout(params['dropout']))
  model.add(Flatten())
  model.add(Dense(num_classes, activation='softmax'))
  model.compile(optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),
                  loss=params['loss'],
                  metrics=['acc'])

  #Class weights to weigh up for imbalanced validation set
  class_weight = {0: 2,
                  1: 1.5,
                  2: 2,
                  3: 1.0}

  out = model.fit(x_train, y_train, 
                        validation_data=[x_val, y_val],
                        batch_size=params['batch_size'],
                        epochs=params['epochs'],
                        verbose=0,
                        class_weight=class_weight)

  return out, model

################################
#Use hyperparameter optimization for the model

#!pip install talos
import talos as ta
from keras.optimizers import Adam, Nadam
from keras.activations import softmax
from keras.losses import categorical_crossentropy, logcosh
p = {#already optimized values
    'epochs': (7,16,10),
    'optimizer': [Adam, Nadam],#have approximately the same performance
    'batch_size': ([1000]),
    'dropout': (0.1, 0.40, 10),
    'lr': (1, 10, 100),
    'weight_regulizer':[None],
    'filters_first_conv':[128],   
    'filters_second_conv':[32],
    'last_activation': [softmax],
    'loss': [logcosh],#use logcosh, as it works far better with imbalanced data set
}

h = ta.Scan(train_features, train_labels, 
            x_val = valid_features,
            y_val = valid_labels,
            params=p,
            model=dl_model,
            dataset_name='dl',
            experiment_no='1',
            grid_downsample=.01)

################################
# Generate a report
r = ta.Reporting(h)
# return the highest value for the validation accuracy and the best hyperparams to achieve it
print(r.high('val_acc'))
print(r.best_params('val_acc',3))

################################
# compute confusion matrixes

from sklearn.metrics import confusion_matrix
import numpy as np
np.set_printoptions(precision=3, suppress=True)

p = ta.Predict(h)

#Confusion matrix and class distribution for train data
train_classes = p.predict_classes(train_features)
train_matrix = confusion_matrix(train_classes,train_labels_int)
print("\nConfusion matrix for classifying train data:")
print(train_matrix)
print("\nDistribution of true labels for train data:")
train_dist = np.sum(train_matrix,axis=0)
print(train_dist/sum(train_dist))
print("\nDistribution of predicted labels for train data:")
train_dist_pred = np.sum(train_matrix,axis=1)
print(train_dist_pred/sum(train_dist_pred))


#Confusion matrix for validation data
valid_classes = p.predict_classes(valid_features)
valid_matrix = confusion_matrix(valid_classes,valid_labels_int)
print("\n\nConfusion matrix for classifying validation data:")
print(valid_matrix)
print("\nDistribution of true labels for validation data:")
valid_dist = np.sum(valid_matrix,axis=0)
print(valid_dist/sum(valid_dist))
print("\nDistribution of predicted labels for validation data:")
valid_dist_pred = np.sum(valid_matrix,axis=1)
print(valid_dist_pred/sum(valid_dist_pred))

# Distribution of classified labels for test data
test_classes = p.predict_classes(test_features)
print("\n\nDistribution of predicted labels for test data:")
test_dist = [sum(test_classes == label) for label in range(4)]
print(test_dist/sum(test_dist))

#Save in file
translateLabels = ['angry','happy','sad','neutral']

with open("3324181_ebner_topic2_result.txt",'w') as wFile:
  for i in range(len(test_ids)):
    strToWrite = test_ids[i].decode('UTF-8') + " " + translateLabels[test_classes[i]] + "\n"
    wFile.write(strToWrite)