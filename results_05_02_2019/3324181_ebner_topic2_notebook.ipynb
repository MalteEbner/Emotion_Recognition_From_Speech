{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl_attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bnmtAHCVZ7Nh",
        "colab_type": "code",
        "outputId": "70214f00-17c8-4dc9-c8e1-001f96832103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "cell_type": "code",
      "source": [
        "################################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UqTZvBiDds_6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################\n",
        "# save data in better form \n",
        "#(only once, only relevant data, concat features from logmel and prosody)\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "##dictionary for translating labels\n",
        "labelDic = {'angry': 0, 'happy': 1, 'sad': 2, 'neutral': 3}\n",
        "\n",
        "# Load logmel for training\n",
        "f = open(\"data_logmel.train\", \"rb\")\n",
        "pickle.load(f)  # ignore ids\n",
        "train_features = pickle.load(f)\n",
        "train_labels = pickle.load(f)\n",
        "train_labels = [labelDic[label] for label in train_labels]\n",
        "f.close()\n",
        "# load prosody for training\n",
        "f = open(\"data_prosody.train\", \"rb\")\n",
        "pickle.load(f)  # ignore ids\n",
        "train_features = np.concatenate((train_features, pickle.load(f)), axis=2)\n",
        "pickle.load(f)  # ignore labels, they are the same\n",
        "f.close()\n",
        "\n",
        "# Load logmel for validation\n",
        "f = open(\"data_logmel.valid\", \"rb\")\n",
        "pickle.load(f)  # ignore ids\n",
        "valid_features = pickle.load(f)\n",
        "valid_labels = pickle.load(f)\n",
        "valid_labels = [labelDic[label] for label in valid_labels]\n",
        "f.close()\n",
        "# load prosody for validation\n",
        "f = open(\"data_prosody.valid\", \"rb\")\n",
        "pickle.load(f)  # ignore ids\n",
        "valid_features = np.concatenate((valid_features, pickle.load(f)), axis=2)\n",
        "pickle.load(f)  # ignore labels, they are the same\n",
        "f.close()\n",
        "\n",
        "# Load logmel for testing\n",
        "f = open(\"data_logmel.test\", \"rb\")\n",
        "pickle.load(f)  # ignore ids\n",
        "test_features = pickle.load(f)\n",
        "f.close()\n",
        "# load prosody for testing\n",
        "f = open(\"data_prosody.test\", \"rb\")\n",
        "test_ids = pickle.load(f)  # ignore ids\n",
        "test_features = np.concatenate((test_features, pickle.load(f)), axis=2)\n",
        "f.close()\n",
        "\n",
        "# save in better form\n",
        "with open(\"data_project\", 'wb') as f_out:\n",
        "    pickle.dump([train_features, train_labels, valid_features, valid_labels, test_features, test_ids], f_out)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FOnD4ycGVJk-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################\n",
        "# define dimensions\n",
        "num_frames = 750\n",
        "num_feats = 33\n",
        "num_classes = 4\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P0xugmPwVOmF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "################################\n",
        "# Load the data\n",
        "import pickle \n",
        "import numpy as np\n",
        "\n",
        "loadTrueData = True\n",
        "if loadTrueData:\n",
        "    f = open('/content/gdrive/My Drive/DL_project/data_project', \"rb\")\n",
        "    [train_features_3dim, train_labels_int, valid_features_3dim, valid_labels_int, test_features_3dim, test_ids] = pickle.load(f,encoding='latin1')\n",
        "else:\n",
        "    noSamples = 20\n",
        "    train_features_3dim = np.ones((noSamples,num_frames,num_feats))\n",
        "    train_labels_int = np.random.randint(num_classes, size = noSamples)\n",
        "    valid_features_3dim = np.ones((noSamples,num_frames))\n",
        "    valid_labels_int = np.random.randint(num_classes, size = noSamples)\n",
        "    test_features_3dim = np.ones((noSamples,num_frames))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x0wd041ai1oM",
        "colab_type": "code",
        "outputId": "42b48cd8-c45c-432c-b572-8d7177dbc37c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "################################\n",
        "# prepare the data\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "train_labels = to_categorical(train_labels_int)\n",
        "valid_labels = to_categorical(valid_labels_int)\n",
        "train_features = train_features_3dim[..., np.newaxis]\n",
        "valid_features = valid_features_3dim[..., np.newaxis]\n",
        "test_features = test_features_3dim[..., np.newaxis]\n",
        "fulltrain_features = np.concatenate((train_features,valid_features),axis = 0)\n",
        "fulltrain_labels = np.concatenate((train_labels,valid_labels),axis = 0)\n",
        "\n",
        "print(train_features.shape)\n",
        "print( train_labels.shape)\n",
        "print( valid_features.shape)\n",
        "print( valid_labels.shape)\n",
        "print( test_features.shape)\n",
        "\n",
        "print( fulltrain_features.shape)\n",
        "print( fulltrain_labels.shape)\n",
        "\n",
        "def accuracy(a,b):\n",
        "  prod = a*b\n",
        "  sh = prod.shape\n",
        "  return sum(sum(prod))/sh[0]\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5531, 750, 33, 1)\n",
            "(5531, 4)\n",
            "(3898, 750, 33, 1)\n",
            "(3898, 4)\n",
            "(3900, 750, 33, 1)\n",
            "(9429, 750, 33, 1)\n",
            "(9429, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k1HzwlQJOnNJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################\n",
        "# Create a model\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Multiply, Dense, Conv2D, Flatten, Dropout, BatchNormalization, Activation, MaxPooling2D\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.losses import categorical_crossentropy\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import sys\n",
        "\n",
        "\n",
        "def dl_model(x_train, y_train, x_val, y_val,params,y_val_int):\n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  inputs = Input(shape=(num_frames,num_feats,1))\n",
        "  \n",
        "  #first convolutional layer\n",
        "  conv1 = Conv2D(params['filters_first_conv'], kernel_size=(12,12), strides=(7,7),activation='relu')(inputs)\n",
        "  conv1 = MaxPooling2D(pool_size=(int(params['poolingSize']), 1), strides=None, padding='valid', data_format=None)(conv1)\n",
        "  conv1 = Dropout(params['dropout1'])(conv1)\n",
        "  conv1_flattened = Flatten()(conv1)\n",
        "\n",
        "  #model for getting size of last layer\n",
        "  tempModel = Model(input=[inputs], output=conv1_flattened)\n",
        "  shapeLastLayer = tempModel.layers[-1].output_shape\n",
        "  \n",
        "  #attention layer\n",
        "  #ks_2 = int(params['conv2_kernelSize'])\n",
        "  #stride_2 = int(0.6*ks_2)\n",
        "  #attention_conv2 = Conv2D(params['filters_second_conv'], kernel_size=(ks_2,1), strides=(stride_2,1),activation='relu')(conv1)\n",
        "  #attention_conv2 = Flatten()(attention_conv2)\n",
        "  #attention_probs = Dense(shapeLastLayer[1], activation='softmax', name='attention_vec')(attention_conv2)\n",
        "  \n",
        "  #Merge layers\n",
        "  #attention_mul = Multiply()([conv1_flattened, attention_probs])\n",
        "  \n",
        "  #hidden layer\n",
        "  dense1 = Dense(params['noNeuronsDense'])(conv1_flattened)\n",
        "  #dense1 = Dense(params['noNeuronsDense'])(attention_mul)\n",
        "  dense1 = Activation('relu')(dense1)\n",
        "  dense1 = BatchNormalization()(dense1)\n",
        "  dense1 = Dropout(params['dropout3'])(dense1)\n",
        "  \n",
        "  #output layer\n",
        "  output = Dense(num_classes, activation='softmax')(dense1)\n",
        "\n",
        "  \n",
        "  #Model\n",
        "  model = Model(input=[inputs], output=output)\n",
        "  #model.summary()\n",
        "  #sgd = optimizers.SGD(params['learningRate'], decay=1e-6, nesterov=True)\n",
        "  model.compile(optimizer=params['optimizer'],\n",
        "                  loss=categorical_crossentropy,\n",
        "                  metrics=['acc'])\n",
        "\n",
        "  #Class weights to weigh up for imbalanced validation set\n",
        "  class_weight = {0: 0,\n",
        "                  1: params['classWeight1'],\n",
        "                  2: 0,\n",
        "                  3: 1}\n",
        "  \n",
        "  if ITERATION==2:\n",
        "    plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "  \n",
        "  \n",
        "  history = model.fit(x_train, y_train, \n",
        "                        validation_data=[x_val, y_val],\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=int(params['epochs']),\n",
        "                        verbose=0,\n",
        "                        class_weight = class_weight)\n",
        "  \n",
        "  \n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gPY9FrAePpEU",
        "colab_type": "code",
        "outputId": "daf66913-48c3-4392-abce-257e1c1bfb8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3182
        }
      },
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "#Do hyperparameter optimization\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from keras.optimizers import Adadelta, Adam, rmsprop\n",
        "from keras.activations import softmax\n",
        "from keras.losses import categorical_crossentropy, logcosh\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from timeit import default_timer as timer\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Global variables\n",
        "global  ITERATION\n",
        "ITERATION = 0\n",
        "global out_file\n",
        "out_file = \"results.csv\"\n",
        "# File to save first results\n",
        "of_connection = open(out_file, 'w')\n",
        "writer = csv.writer(of_connection)\n",
        "# Write the headers to the file\n",
        "writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\n",
        "of_connection.close()\n",
        "\n",
        "#Define Function needed for optimizer\n",
        "def optimize_model(x_train, y_train, x_val, y_val, params,y_val_int):\n",
        "  # Keep track of evals\n",
        "  global ITERATION\n",
        "  ITERATION += 1\n",
        "  start = timer()\n",
        "  \n",
        "  trainedModel = dl_model(x_train, y_train, x_val, y_val,params,y_val_int)\n",
        "  probs = trainedModel.predict(x_val, batch_size = 512, verbose = 0)\n",
        "  y_pred = np.argmax(probs,axis=1) \n",
        "  acc = accuracy_score(y_val_int, y_pred)\n",
        "  print('\\naccuracy:', acc, ', iter: ', ITERATION, ', confusion matrix:')  \n",
        "  conf_matrix = confusion_matrix(y_pred,y_val_int)\n",
        "  print(conf_matrix)\n",
        "  sys.stdout.flush() \n",
        "  \n",
        "  run_time = timer() - start\n",
        "  \n",
        "  # Write to the csv file ('a' means append)\n",
        "\n",
        "  of_connection = open(out_file, 'a')\n",
        "  writer = csv.writer(of_connection)\n",
        "  writer.writerow([acc, params, ITERATION, run_time])\n",
        "  \n",
        "  \n",
        "  return {'loss': -acc, 'status': STATUS_OK}\n",
        "\n",
        "space = {\n",
        "    'epochs': hp.quniform('epochs', 10,15,1),\n",
        "    'poolingSize': hp.quniform('poolingSize', 10,15,1),\n",
        "    \n",
        "    #'learningRate': hp.uniform('learningRate',0.03,0.03),\n",
        "    #'momentum': hp.uniform('momentum',0.7,0.95),\n",
        "    'optimizer': hp.choice('optimizer',['adam']),\n",
        "    'batch_size': hp.choice('batch_size', [512]),\n",
        "    'dropout1': hp.uniform('dropout1', 0.1,0.3),\n",
        "    #'dropout2': hp.uniform('dropout2', 0.0,0.4),\n",
        "    'dropout3': hp.uniform('dropout3', 0.1,0.3),\n",
        "    'noNeuronsDense': hp.choice('noNeuronsDense', [24]),\n",
        "    #'classWeight0': hp.uniform('classWeight0', 0.0,0.02),\n",
        "    'classWeight1': hp.uniform('classWeight1', 0.8,1.2),\n",
        "    #'classWeight2': hp.uniform('classWeight2', 0.0,0.02),\n",
        "    'weight_regulizer':hp.uniform('weight_regularizer', 0.1,1),\n",
        "    'filters_first_conv':hp.choice('filters_first_conv', [48]),   \n",
        "    #'filters_second_conv':hp.choice('filters_second_conv', [32, 48]),\n",
        "    #'conv2_kernelSize': hp.quniform('conv2_kernelSize',2,5,1)\n",
        "    #'loss': hp.choice('loss', [categorical_crossentropy]),\n",
        "        }\n",
        "  \n",
        "objective = lambda params: optimize_model(train_features,train_labels,valid_features,valid_labels,params,valid_labels_int) \n",
        "trials = Trials()\n",
        "bestParamsRaw = fmin(objective, space, algo=tpe.suggest, max_evals=30, trials=trials)\n",
        "\n",
        "print('FINISHED')\n",
        "\n",
        "  "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"fl...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "accuracy: 0.4976911236531555 , iter:  1 , confusion matrix:\n",
            "[[   7    9    5    3]\n",
            " [ 247  577   98  381]\n",
            " [   1    6    7    5]\n",
            " [ 141  730  332 1349]]\n",
            "\n",
            "accuracy: 0.4787070292457671 , iter:  2 , confusion matrix:\n",
            "[[   0    0    0    1]\n",
            " [ 285  772  179  641]\n",
            " [   1    2    0    2]\n",
            " [ 110  548  263 1094]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"fl...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "accuracy: 0.4843509492047204 , iter:  3 , confusion matrix:\n",
            "[[   1    2    1    1]\n",
            " [ 133  337   47  183]\n",
            " [   1    2    0    4]\n",
            " [ 261  981  394 1550]]\n",
            "\n",
            "accuracy: 0.482811698306824 , iter:  4 , confusion matrix:\n",
            "[[   2    5    3    8]\n",
            " [ 253  667  133  517]\n",
            " [   0    0    0    0]\n",
            " [ 141  650  306 1213]]\n",
            "\n",
            "accuracy: 0.4820420728578758 , iter:  5 , confusion matrix:\n",
            "[[   3    6    1    0]\n",
            " [ 186  433   56  289]\n",
            " [   5   13    5   11]\n",
            " [ 202  870  380 1438]]\n",
            "\n",
            "accuracy: 0.4856336582863007 , iter:  6 , confusion matrix:\n",
            "[[   2    1    1    3]\n",
            " [ 232  565  101  404]\n",
            " [   0    0    0    5]\n",
            " [ 162  756  340 1326]]\n",
            "\n",
            "accuracy: 0.4835813237557722 , iter:  7 , confusion matrix:\n",
            "[[   0    0    0    0]\n",
            " [ 259  686  149  539]\n",
            " [   2   10    2    2]\n",
            " [ 135  626  291 1197]]\n",
            "\n",
            "accuracy: 0.4943560800410467 , iter:  8 , confusion matrix:\n",
            "[[   0    4    2    1]\n",
            " [ 266  708  139  511]\n",
            " [   2    1    3   10]\n",
            " [ 128  609  298 1216]]\n",
            "\n",
            "accuracy: 0.478450487429451 , iter:  9 , confusion matrix:\n",
            "[[   0    2    1    0]\n",
            " [ 278  680  149  549]\n",
            " [   0    2    1    5]\n",
            " [ 118  638  291 1184]]\n",
            "\n",
            "accuracy: 0.4861467419189328 , iter:  10 , confusion matrix:\n",
            "[[   0    3    2    2]\n",
            " [ 241  606  105  447]\n",
            " [   0    2    0    0]\n",
            " [ 155  711  335 1289]]\n",
            "\n",
            "accuracy: 0.48819907644946126 , iter:  11 , confusion matrix:\n",
            "[[   1    1    1    1]\n",
            " [ 265  648  128  483]\n",
            " [   2    0    0    0]\n",
            " [ 128  673  313 1254]]\n",
            "\n",
            "accuracy: 0.49409953822473063 , iter:  12 , confusion matrix:\n",
            "[[   1    8    1    1]\n",
            " [ 228  559   95  372]\n",
            " [   2    2    1    0]\n",
            " [ 165  753  345 1365]]\n",
            "\n",
            "accuracy: 0.48024628014366344 , iter:  13 , confusion matrix:\n",
            "[[   0    0    0    0]\n",
            " [ 229  580  120  446]\n",
            " [   3    8    1    1]\n",
            " [ 164  734  321 1291]]\n",
            "\n",
            "accuracy: 0.48024628014366344 , iter:  14 , confusion matrix:\n",
            "[[   0    3    1    1]\n",
            " [ 136  352   44  212]\n",
            " [  19   23    9   14]\n",
            " [ 241  944  388 1511]]\n",
            "\n",
            "accuracy: 0.48896870189840946 , iter:  15 , confusion matrix:\n",
            "[[   2    8    2    6]\n",
            " [ 213  553   88  381]\n",
            " [   0    1    2    2]\n",
            " [ 181  760  350 1349]]\n",
            "\n",
            "accuracy: 0.48486403283735247 , iter:  16 , confusion matrix:\n",
            "[[   0    0    0    0]\n",
            " [ 238  601  108  451]\n",
            " [   5    3    3    1]\n",
            " [ 153  718  331 1286]]\n",
            "\n",
            "accuracy: 0.4710107747562853 , iter:  17 , confusion matrix:\n",
            "[[   0    0    0    1]\n",
            " [ 253  599  137  495]\n",
            " [   1    3    1    6]\n",
            " [ 142  720  304 1236]]\n",
            "\n",
            "accuracy: 0.47819394561313494 , iter:  18 , confusion matrix:\n",
            "[[   0    0    0    0]\n",
            " [ 232  574  109  447]\n",
            " [   6   11    7    8]\n",
            " [ 158  737  326 1283]]\n",
            "\n",
            "accuracy: 0.49025141097998975 , iter:  19 , confusion matrix:\n",
            "[[   1    4    0    3]\n",
            " [ 202  512   72  340]\n",
            " [  10   15    7    4]\n",
            " [ 183  791  363 1391]]\n",
            "\n",
            "accuracy: 0.4776808619805028 , iter:  20 , confusion matrix:\n",
            "[[   4    7    2   22]\n",
            " [ 243  610  145  468]\n",
            " [   0    1    0    0]\n",
            " [ 149  704  295 1248]]\n",
            "\n",
            "accuracy: 0.4830682401231401 , iter:  21 , confusion matrix:\n",
            "[[  12   12    7   16]\n",
            " [ 243  617  120  468]\n",
            " [   0    1    2    2]\n",
            " [ 141  692  313 1252]]\n",
            "\n",
            "accuracy: 0.48512057465366853 , iter:  22 , confusion matrix:\n",
            "[[   2    8    0   11]\n",
            " [ 250  617  120  454]\n",
            " [   1    3    1    2]\n",
            " [ 143  694  321 1271]]\n",
            "\n",
            "accuracy: 0.4876859928168291 , iter:  23 , confusion matrix:\n",
            "[[   3    2    3    0]\n",
            " [ 245  636  125  469]\n",
            " [   2    7    1    8]\n",
            " [ 146  677  313 1261]]\n",
            "\n",
            "accuracy: 0.47562852744997436 , iter:  24 , confusion matrix:\n",
            "[[   1    0    1    6]\n",
            " [ 239  569  110  446]\n",
            " [   0    0    1    3]\n",
            " [ 156  753  330 1283]]\n",
            "\n",
            "accuracy: 0.4797331965110313 , iter:  25 , confusion matrix:\n",
            "[[   2    9    3   14]\n",
            " [ 192  447   80  303]\n",
            " [   1    1    0    0]\n",
            " [ 201  865  359 1421]]\n",
            "\n",
            "accuracy: 0.48691636736788096 , iter:  26 , confusion matrix:\n",
            "[[   2    7    3    3]\n",
            " [ 278  723  141  558]\n",
            " [   0    0    0    4]\n",
            " [ 116  592  298 1173]]\n",
            "\n",
            "accuracy: 0.4856336582863007 , iter:  27 , confusion matrix:\n",
            "[[   2    4    0    1]\n",
            " [ 226  553   90  389]\n",
            " [   6   18    6   16]\n",
            " [ 162  747  346 1332]]\n",
            "\n",
            "accuracy: 0.48512057465366853 , iter:  28 , confusion matrix:\n",
            "[[   6   11    3    9]\n",
            " [ 198  480   73  317]\n",
            " [   1    1    1    8]\n",
            " [ 191  830  365 1404]]\n",
            "\n",
            "accuracy: 0.48255515649050795 , iter:  29 , confusion matrix:\n",
            "[[   0    0    1    4]\n",
            " [ 218  542   85  391]\n",
            " [   3    2    2    6]\n",
            " [ 175  778  354 1337]]\n",
            "\n",
            "accuracy: 0.48819907644946126 , iter:  30 , confusion matrix:\n",
            "[[  10    7    1    6]\n",
            " [ 200  486   84  322]\n",
            " [   6   12    1    4]\n",
            " [ 180  817  356 1406]]\n",
            "FINISHED\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Szhx9R8l7Ost",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "e53d06ac-f158-434f-d5d9-781cc63c4f66"
      },
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# show trials with performance and params in sorted table\n",
        "\n",
        "import ast\n",
        "\n",
        "# Sort the trials with lowest loss (highest AUC) first\n",
        "trials_results = sorted(trials.results, key = lambda x: x['loss'])\n",
        "\n",
        "#read file\n",
        "results = pd.read_csv(out_file)\n",
        "# Sort with best scores on top and reset index for slicing\n",
        "results.sort_values('loss', ascending = False, inplace = True)\n",
        "results.reset_index(inplace = True, drop = True)\n",
        "\n",
        "# Create a new dataframe for storing parameters\n",
        "bayes_params = pd.DataFrame(columns = list(ast.literal_eval(results.loc[0, 'params']).keys()),\n",
        "                            index = list(range(len(results))))\n",
        "# Add the results with each parameter a different column\n",
        "for i, params in enumerate(results['params']):\n",
        "    bayes_params.loc[i, :] = list(ast.literal_eval(params).values())   \n",
        "bayes_params['acc'] = results['loss']\n",
        "bayes_params['iteration'] = results['iteration']\n",
        "print(bayes_params.head(10))\n",
        "\n",
        "#get best hyperparameters\n",
        "from hyperopt import space_eval\n",
        "bestParams = space_eval(space, trials.argmin)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  batch_size classWeight1  dropout1  dropout3 epochs filters_first_conv  \\\n",
            "0        512       1.0598  0.163829  0.296926     11                 48   \n",
            "1        512     0.816718  0.149886  0.183898     11                 48   \n",
            "2        512      1.00469  0.100638  0.121903     14                 48   \n",
            "3        512     0.819322  0.233692  0.250844     12                 48   \n",
            "4        512      1.06541  0.279584  0.231624     11                 48   \n",
            "5        512       1.0748  0.235999  0.169461     15                 48   \n",
            "6        512      1.19669  0.147109  0.101973     10                 48   \n",
            "7        512      1.13239  0.205359  0.167776     13                 48   \n",
            "8        512     0.927701   0.25346  0.222832     10                 48   \n",
            "9        512      1.05558  0.118918  0.291989     10                 48   \n",
            "\n",
            "  noNeuronsDense optimizer poolingSize weight_regulizer       acc  iteration  \n",
            "0             24      adam          11         0.147189  0.497691          1  \n",
            "1             24      adam          12         0.439547  0.494356          8  \n",
            "2             24      adam          11         0.203535  0.494100         12  \n",
            "3             24      adam          13         0.794362  0.490251         19  \n",
            "4             24      adam          15         0.916999  0.488969         15  \n",
            "5             24      adam          11         0.392189  0.488199         11  \n",
            "6             24      adam          10         0.882414  0.488199         30  \n",
            "7             24      adam          10         0.696772  0.487686         23  \n",
            "8             24      adam          11          0.30835  0.486916         26  \n",
            "9             24      adam          12         0.947905  0.486147         10  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5TLRJ0F_NFyY",
        "colab_type": "code",
        "outputId": "4456b92c-398a-43b3-aa8b-d068b424a419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      },
      "cell_type": "code",
      "source": [
        "#do final training on best hyperparams and define prediction function\n",
        "trainedModel = dl_model(fulltrain_features,fulltrain_labels,valid_features,valid_labels,bestParams,valid_labels_int)\n",
        "predict = lambda x: np.argmax(trainedModel.predict(x, batch_size = 512, verbose = 0),axis=1)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"fl...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "gBtMDU8pUx-c",
        "colab_type": "code",
        "outputId": "9d09ee5b-6bae-42ff-9fd5-79cf88922536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "cell_type": "code",
      "source": [
        "################################\n",
        "# compute confusion matrices\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "\n",
        "#Confusion matrix and class distribution for train data\n",
        "train_classes = predict(train_features)\n",
        "train_matrix = confusion_matrix(train_classes,train_labels_int)\n",
        "print(\"\\nConfusion matrix for classifying train data:\")\n",
        "print(train_matrix)\n",
        "print(\"\\nDistribution of true labels for train data:\")\n",
        "train_dist = np.sum(train_matrix,axis=0)\n",
        "print(train_dist/sum(train_dist))\n",
        "print(\"\\nDistribution of predicted labels for train data:\")\n",
        "train_dist_pred = np.sum(train_matrix,axis=1)\n",
        "print(train_dist_pred/sum(train_dist_pred))\n",
        "print(\"\\nAccuracy on train data:\")\n",
        "print(accuracy_score(train_classes,train_labels_int))\n",
        "\n",
        "\n",
        "#Confusion matrix for validation data\n",
        "valid_classes = predict(valid_features)\n",
        "valid_matrix = confusion_matrix(valid_classes,valid_labels_int)\n",
        "print(\"\\n\\nConfusion matrix for classifying validation data:\")\n",
        "print(valid_matrix)\n",
        "print(\"\\nDistribution of true labels for validation data:\")\n",
        "valid_dist = np.sum(valid_matrix,axis=0)\n",
        "print(valid_dist/sum(valid_dist))\n",
        "print(\"\\nDistribution of predicted labels for validation data:\")\n",
        "valid_dist_pred = np.sum(valid_matrix,axis=1)\n",
        "print(valid_dist_pred/sum(valid_dist_pred))\n",
        "print(\"\\nAccuracy on validation data:\")\n",
        "print(accuracy_score(valid_classes,valid_labels_int))\n",
        "\n",
        "# Distribution of classified labels for test data\n",
        "test_classes = predict(test_features)\n",
        "print(\"\\n\\nDistribution of predicted labels for test data:\")\n",
        "test_dist = [sum(test_classes == label) for label in range(4)]\n",
        "print(test_dist/sum(test_dist))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Confusion matrix for classifying train data:\n",
            "[[   1    0    3    0]\n",
            " [ 654  793  103  137]\n",
            " [  17    0    0    0]\n",
            " [ 431  843  978 1571]]\n",
            "\n",
            "Distribution of true labels for train data:\n",
            "[0.199 0.296 0.196 0.309]\n",
            "\n",
            "Distribution of predicted labels for train data:\n",
            "[0.001 0.305 0.003 0.691]\n",
            "\n",
            "Accuracy on train data:\n",
            "0.4275899475682517\n",
            "\n",
            "\n",
            "Confusion matrix for classifying validation data:\n",
            "[[   0    0    2    0]\n",
            " [ 173  515   63  174]\n",
            " [   0    0    0    0]\n",
            " [ 223  807  377 1564]]\n",
            "\n",
            "Distribution of true labels for validation data:\n",
            "[0.102 0.339 0.113 0.446]\n",
            "\n",
            "Distribution of predicted labels for validation data:\n",
            "[0.001 0.237 0.    0.762]\n",
            "\n",
            "Accuracy on validation data:\n",
            "0.5333504361210878\n",
            "\n",
            "\n",
            "Distribution of predicted labels for test data:\n",
            "[0.    0.288 0.    0.712]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "70kHA6fPX3ed",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################\n",
        "#Save labels in file\n",
        "translateLabels = ['angry','happy','sad','neutral']\n",
        "\n",
        "with open(\"3324181_ebner_topic2_result.txt\",'w') as wFile:\n",
        "  for i in range(len(test_ids)):\n",
        "    strToWrite = test_ids[i].decode('UTF-8') + \" \" + translateLabels[test_classes[i]] + \"\\n\"\n",
        "    wFile.write(strToWrite)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}